# Music Informer
This repository contains the implementation of Music Informer, a model for music generation based on the architecture of Music Transformer. While Music Informer inherits key components such as the relative attention mechanism from Music Transformer, it introduces several novel features, including: ProbSparse Self-Attention Mechanism: Reduces computational complexity by selectively attending to the most important tokens.LSTM Structure: Enhances the model's ability to capture long-term dependencies in musical sequences.These improvements enable Music Informer to generate music with higher coherence and overall quality compared to its predecessor.
Currently, the implementation supports PyTorch >= 1.2.0 with Python >= 3.6.


## Generated Music:
Recordings generated by Music Informer can be accessed at https://github.com/1Garlicboy/MusicInformer/tree/main/samples.


### Training
```
python train.py -output_dir rpr --rpr 
```
You can additonally specify both a weight and print modulus that determine what epochs to save weights and what batches to print. The weights that achieved the best loss and the best accuracy (separate) are always stored in results, regardless of weight modulus input.

### Evaluation
```
python evaluate.py -model_weights results/music_informer/best_acc_weights.pickle --rpr
```

Your model's results may vary because a random sequence start position is chosen for each evaluation piece. This may be changed in the future.

### Generation
```
python generate.py -output_dir output -model_weights results/music_informer/best_acc_weights.pickle --rpr
```

## Results
We trained a base model with the following parameters for 200 epochs:
* **learn_rate**: None
* **ce_smoothing**: None
* **batch_size**: 2
* **max_sequence**: 2048
* **n_layers**: 6
* **num_heads**: 8
* **d_model**: 512
* **dim_feedforward**: 1024
* **dropout**: 0.1






